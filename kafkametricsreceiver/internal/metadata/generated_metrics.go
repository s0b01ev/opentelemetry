// Code generated by mdatagen. DO NOT EDIT.

package metadata

import (
	"time"

	"go.opentelemetry.io/collector/component"
	"go.opentelemetry.io/collector/pdata/pcommon"
	"go.opentelemetry.io/collector/pdata/pmetric"
)

// MetricSettings provides common settings for a particular metric.
type MetricSettings struct {
	Enabled bool `mapstructure:"enabled"`
}

// MetricsSettings provides settings for kafkametricsreceiver metrics.
type MetricsSettings struct {
	KafkaBrokers                     MetricSettings `mapstructure:"kafka.brokers"`
	KafkaBrokersConsumerFetchRateAvg MetricSettings `mapstructure:"kafka.brokers.consumer_fetch_rate_avg"`
	KafkaBrokersIncomingByteRateAvg  MetricSettings `mapstructure:"kafka.brokers.incoming_byte_rate_avg"`
	KafkaBrokersOutgoingByteRateAvg  MetricSettings `mapstructure:"kafka.brokers.outgoing_byte_rate_avg"`
	KafkaBrokersRequestLatencyAvg    MetricSettings `mapstructure:"kafka.brokers.request_latency_avg"`
	KafkaBrokersRequestRateAvg       MetricSettings `mapstructure:"kafka.brokers.request_rate_avg"`
	KafkaBrokersRequestSizeAvg       MetricSettings `mapstructure:"kafka.brokers.request_size_avg"`
	KafkaBrokersRequestsInFlight     MetricSettings `mapstructure:"kafka.brokers.requests_in_flight"`
	KafkaBrokersResponseRateAvg      MetricSettings `mapstructure:"kafka.brokers.response_rate_avg"`
	KafkaBrokersResponseSizeAvg      MetricSettings `mapstructure:"kafka.brokers.response_size_avg"`
	KafkaConsumerGroupLag            MetricSettings `mapstructure:"kafka.consumer_group.lag"`
	KafkaConsumerGroupLagSum         MetricSettings `mapstructure:"kafka.consumer_group.lag_sum"`
	KafkaConsumerGroupMembers        MetricSettings `mapstructure:"kafka.consumer_group.members"`
	KafkaConsumerGroupOffset         MetricSettings `mapstructure:"kafka.consumer_group.offset"`
	KafkaConsumerGroupOffsetSum      MetricSettings `mapstructure:"kafka.consumer_group.offset_sum"`
	KafkaPartitionCurrentOffset      MetricSettings `mapstructure:"kafka.partition.current_offset"`
	KafkaPartitionOldestOffset       MetricSettings `mapstructure:"kafka.partition.oldest_offset"`
	KafkaPartitionReplicas           MetricSettings `mapstructure:"kafka.partition.replicas"`
	KafkaPartitionReplicasInSync     MetricSettings `mapstructure:"kafka.partition.replicas_in_sync"`
	KafkaTopicPartitions             MetricSettings `mapstructure:"kafka.topic.partitions"`
}

func DefaultMetricsSettings() MetricsSettings {
	return MetricsSettings{
		KafkaBrokers: MetricSettings{
			Enabled: true,
		},
		KafkaBrokersConsumerFetchRateAvg: MetricSettings{
			Enabled: true,
		},
		KafkaBrokersIncomingByteRateAvg: MetricSettings{
			Enabled: true,
		},
		KafkaBrokersOutgoingByteRateAvg: MetricSettings{
			Enabled: true,
		},
		KafkaBrokersRequestLatencyAvg: MetricSettings{
			Enabled: true,
		},
		KafkaBrokersRequestRateAvg: MetricSettings{
			Enabled: true,
		},
		KafkaBrokersRequestSizeAvg: MetricSettings{
			Enabled: true,
		},
		KafkaBrokersRequestsInFlight: MetricSettings{
			Enabled: true,
		},
		KafkaBrokersResponseRateAvg: MetricSettings{
			Enabled: true,
		},
		KafkaBrokersResponseSizeAvg: MetricSettings{
			Enabled: true,
		},
		KafkaConsumerGroupLag: MetricSettings{
			Enabled: true,
		},
		KafkaConsumerGroupLagSum: MetricSettings{
			Enabled: true,
		},
		KafkaConsumerGroupMembers: MetricSettings{
			Enabled: true,
		},
		KafkaConsumerGroupOffset: MetricSettings{
			Enabled: true,
		},
		KafkaConsumerGroupOffsetSum: MetricSettings{
			Enabled: true,
		},
		KafkaPartitionCurrentOffset: MetricSettings{
			Enabled: true,
		},
		KafkaPartitionOldestOffset: MetricSettings{
			Enabled: true,
		},
		KafkaPartitionReplicas: MetricSettings{
			Enabled: true,
		},
		KafkaPartitionReplicasInSync: MetricSettings{
			Enabled: true,
		},
		KafkaTopicPartitions: MetricSettings{
			Enabled: true,
		},
	}
}

type metricKafkaBrokers struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.brokers metric with initial data.
func (m *metricKafkaBrokers) init() {
	m.data.SetName("kafka.brokers")
	m.data.SetDescription("Number of brokers in the cluster.")
	m.data.SetUnit("{brokers}")
	m.data.SetEmptyGauge()
}

func (m *metricKafkaBrokers) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaBrokers) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaBrokers) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaBrokers(settings MetricSettings) metricKafkaBrokers {
	m := metricKafkaBrokers{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaBrokersConsumerFetchRateAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.brokers.consumer_fetch_rate_avg metric with initial data.
func (m *metricKafkaBrokersConsumerFetchRateAvg) init() {
	m.data.SetName("kafka.brokers.consumer_fetch_rate_avg")
	m.data.SetDescription("Average consumer fetch Rate")
	m.data.SetUnit("{fetches}/s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaBrokersConsumerFetchRateAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, brokerAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleVal(val)
	dp.Attributes().PutString("broker", brokerAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaBrokersConsumerFetchRateAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaBrokersConsumerFetchRateAvg) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaBrokersConsumerFetchRateAvg(settings MetricSettings) metricKafkaBrokersConsumerFetchRateAvg {
	m := metricKafkaBrokersConsumerFetchRateAvg{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaBrokersIncomingByteRateAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.brokers.incoming_byte_rate_avg metric with initial data.
func (m *metricKafkaBrokersIncomingByteRateAvg) init() {
	m.data.SetName("kafka.brokers.incoming_byte_rate_avg")
	m.data.SetDescription("Average tncoming Byte Rate in bytes/second")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaBrokersIncomingByteRateAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, brokerAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleVal(val)
	dp.Attributes().PutString("broker", brokerAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaBrokersIncomingByteRateAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaBrokersIncomingByteRateAvg) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaBrokersIncomingByteRateAvg(settings MetricSettings) metricKafkaBrokersIncomingByteRateAvg {
	m := metricKafkaBrokersIncomingByteRateAvg{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaBrokersOutgoingByteRateAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.brokers.outgoing_byte_rate_avg metric with initial data.
func (m *metricKafkaBrokersOutgoingByteRateAvg) init() {
	m.data.SetName("kafka.brokers.outgoing_byte_rate_avg")
	m.data.SetDescription("Average outgoing Byte Rate in bytes/second.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaBrokersOutgoingByteRateAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, brokerAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleVal(val)
	dp.Attributes().PutString("broker", brokerAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaBrokersOutgoingByteRateAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaBrokersOutgoingByteRateAvg) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaBrokersOutgoingByteRateAvg(settings MetricSettings) metricKafkaBrokersOutgoingByteRateAvg {
	m := metricKafkaBrokersOutgoingByteRateAvg{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaBrokersRequestLatencyAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.brokers.request_latency_avg metric with initial data.
func (m *metricKafkaBrokersRequestLatencyAvg) init() {
	m.data.SetName("kafka.brokers.request_latency_avg")
	m.data.SetDescription("Request latency Average in ms")
	m.data.SetUnit("ms")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaBrokersRequestLatencyAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, brokerAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleVal(val)
	dp.Attributes().PutString("broker", brokerAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaBrokersRequestLatencyAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaBrokersRequestLatencyAvg) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaBrokersRequestLatencyAvg(settings MetricSettings) metricKafkaBrokersRequestLatencyAvg {
	m := metricKafkaBrokersRequestLatencyAvg{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaBrokersRequestRateAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.brokers.request_rate_avg metric with initial data.
func (m *metricKafkaBrokersRequestRateAvg) init() {
	m.data.SetName("kafka.brokers.request_rate_avg")
	m.data.SetDescription("Average request rate per second.")
	m.data.SetUnit("{requests}/s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaBrokersRequestRateAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, brokerAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleVal(val)
	dp.Attributes().PutString("broker", brokerAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaBrokersRequestRateAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaBrokersRequestRateAvg) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaBrokersRequestRateAvg(settings MetricSettings) metricKafkaBrokersRequestRateAvg {
	m := metricKafkaBrokersRequestRateAvg{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaBrokersRequestSizeAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.brokers.request_size_avg metric with initial data.
func (m *metricKafkaBrokersRequestSizeAvg) init() {
	m.data.SetName("kafka.brokers.request_size_avg")
	m.data.SetDescription("Average request size in bytes")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaBrokersRequestSizeAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, brokerAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleVal(val)
	dp.Attributes().PutString("broker", brokerAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaBrokersRequestSizeAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaBrokersRequestSizeAvg) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaBrokersRequestSizeAvg(settings MetricSettings) metricKafkaBrokersRequestSizeAvg {
	m := metricKafkaBrokersRequestSizeAvg{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaBrokersRequestsInFlight struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.brokers.requests_in_flight metric with initial data.
func (m *metricKafkaBrokersRequestsInFlight) init() {
	m.data.SetName("kafka.brokers.requests_in_flight")
	m.data.SetDescription("Requests in flight")
	m.data.SetUnit("{requests}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaBrokersRequestsInFlight) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, brokerAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().PutString("broker", brokerAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaBrokersRequestsInFlight) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaBrokersRequestsInFlight) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaBrokersRequestsInFlight(settings MetricSettings) metricKafkaBrokersRequestsInFlight {
	m := metricKafkaBrokersRequestsInFlight{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaBrokersResponseRateAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.brokers.response_rate_avg metric with initial data.
func (m *metricKafkaBrokersResponseRateAvg) init() {
	m.data.SetName("kafka.brokers.response_rate_avg")
	m.data.SetDescription("Average response rate per second")
	m.data.SetUnit("{response}/s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaBrokersResponseRateAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, brokerAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleVal(val)
	dp.Attributes().PutString("broker", brokerAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaBrokersResponseRateAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaBrokersResponseRateAvg) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaBrokersResponseRateAvg(settings MetricSettings) metricKafkaBrokersResponseRateAvg {
	m := metricKafkaBrokersResponseRateAvg{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaBrokersResponseSizeAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.brokers.response_size_avg metric with initial data.
func (m *metricKafkaBrokersResponseSizeAvg) init() {
	m.data.SetName("kafka.brokers.response_size_avg")
	m.data.SetDescription("Average response size in bytes")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaBrokersResponseSizeAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, brokerAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleVal(val)
	dp.Attributes().PutString("broker", brokerAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaBrokersResponseSizeAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaBrokersResponseSizeAvg) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaBrokersResponseSizeAvg(settings MetricSettings) metricKafkaBrokersResponseSizeAvg {
	m := metricKafkaBrokersResponseSizeAvg{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaConsumerGroupLag struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.consumer_group.lag metric with initial data.
func (m *metricKafkaConsumerGroupLag) init() {
	m.data.SetName("kafka.consumer_group.lag")
	m.data.SetDescription("Current approximate lag of consumer group at partition of topic")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaConsumerGroupLag) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, groupAttributeValue string, topicAttributeValue string, partitionAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().PutString("group", groupAttributeValue)
	dp.Attributes().PutString("topic", topicAttributeValue)
	dp.Attributes().PutInt("partition", partitionAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaConsumerGroupLag) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaConsumerGroupLag) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaConsumerGroupLag(settings MetricSettings) metricKafkaConsumerGroupLag {
	m := metricKafkaConsumerGroupLag{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaConsumerGroupLagSum struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.consumer_group.lag_sum metric with initial data.
func (m *metricKafkaConsumerGroupLagSum) init() {
	m.data.SetName("kafka.consumer_group.lag_sum")
	m.data.SetDescription("Current approximate sum of consumer group lag across all partitions of topic")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaConsumerGroupLagSum) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, groupAttributeValue string, topicAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().PutString("group", groupAttributeValue)
	dp.Attributes().PutString("topic", topicAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaConsumerGroupLagSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaConsumerGroupLagSum) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaConsumerGroupLagSum(settings MetricSettings) metricKafkaConsumerGroupLagSum {
	m := metricKafkaConsumerGroupLagSum{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaConsumerGroupMembers struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.consumer_group.members metric with initial data.
func (m *metricKafkaConsumerGroupMembers) init() {
	m.data.SetName("kafka.consumer_group.members")
	m.data.SetDescription("Count of members in the consumer group")
	m.data.SetUnit("{members}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaConsumerGroupMembers) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, groupAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().PutString("group", groupAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaConsumerGroupMembers) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaConsumerGroupMembers) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaConsumerGroupMembers(settings MetricSettings) metricKafkaConsumerGroupMembers {
	m := metricKafkaConsumerGroupMembers{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaConsumerGroupOffset struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.consumer_group.offset metric with initial data.
func (m *metricKafkaConsumerGroupOffset) init() {
	m.data.SetName("kafka.consumer_group.offset")
	m.data.SetDescription("Current offset of the consumer group at partition of topic")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaConsumerGroupOffset) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, groupAttributeValue string, topicAttributeValue string, partitionAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().PutString("group", groupAttributeValue)
	dp.Attributes().PutString("topic", topicAttributeValue)
	dp.Attributes().PutInt("partition", partitionAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaConsumerGroupOffset) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaConsumerGroupOffset) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaConsumerGroupOffset(settings MetricSettings) metricKafkaConsumerGroupOffset {
	m := metricKafkaConsumerGroupOffset{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaConsumerGroupOffsetSum struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.consumer_group.offset_sum metric with initial data.
func (m *metricKafkaConsumerGroupOffsetSum) init() {
	m.data.SetName("kafka.consumer_group.offset_sum")
	m.data.SetDescription("Sum of consumer group offset across partitions of topic")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaConsumerGroupOffsetSum) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, groupAttributeValue string, topicAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().PutString("group", groupAttributeValue)
	dp.Attributes().PutString("topic", topicAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaConsumerGroupOffsetSum) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaConsumerGroupOffsetSum) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaConsumerGroupOffsetSum(settings MetricSettings) metricKafkaConsumerGroupOffsetSum {
	m := metricKafkaConsumerGroupOffsetSum{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaPartitionCurrentOffset struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.partition.current_offset metric with initial data.
func (m *metricKafkaPartitionCurrentOffset) init() {
	m.data.SetName("kafka.partition.current_offset")
	m.data.SetDescription("Current offset of partition of topic.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaPartitionCurrentOffset) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, topicAttributeValue string, partitionAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().PutString("topic", topicAttributeValue)
	dp.Attributes().PutInt("partition", partitionAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaPartitionCurrentOffset) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaPartitionCurrentOffset) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaPartitionCurrentOffset(settings MetricSettings) metricKafkaPartitionCurrentOffset {
	m := metricKafkaPartitionCurrentOffset{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaPartitionOldestOffset struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.partition.oldest_offset metric with initial data.
func (m *metricKafkaPartitionOldestOffset) init() {
	m.data.SetName("kafka.partition.oldest_offset")
	m.data.SetDescription("Oldest offset of partition of topic")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaPartitionOldestOffset) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, topicAttributeValue string, partitionAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().PutString("topic", topicAttributeValue)
	dp.Attributes().PutInt("partition", partitionAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaPartitionOldestOffset) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaPartitionOldestOffset) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaPartitionOldestOffset(settings MetricSettings) metricKafkaPartitionOldestOffset {
	m := metricKafkaPartitionOldestOffset{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaPartitionReplicas struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.partition.replicas metric with initial data.
func (m *metricKafkaPartitionReplicas) init() {
	m.data.SetName("kafka.partition.replicas")
	m.data.SetDescription("Number of replicas for partition of topic")
	m.data.SetUnit("{replicas}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaPartitionReplicas) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, topicAttributeValue string, partitionAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().PutString("topic", topicAttributeValue)
	dp.Attributes().PutInt("partition", partitionAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaPartitionReplicas) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaPartitionReplicas) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaPartitionReplicas(settings MetricSettings) metricKafkaPartitionReplicas {
	m := metricKafkaPartitionReplicas{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaPartitionReplicasInSync struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.partition.replicas_in_sync metric with initial data.
func (m *metricKafkaPartitionReplicasInSync) init() {
	m.data.SetName("kafka.partition.replicas_in_sync")
	m.data.SetDescription("Number of synchronized replicas of partition")
	m.data.SetUnit("{replicas}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaPartitionReplicasInSync) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, topicAttributeValue string, partitionAttributeValue int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().PutString("topic", topicAttributeValue)
	dp.Attributes().PutInt("partition", partitionAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaPartitionReplicasInSync) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaPartitionReplicasInSync) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaPartitionReplicasInSync(settings MetricSettings) metricKafkaPartitionReplicasInSync {
	m := metricKafkaPartitionReplicasInSync{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricKafkaTopicPartitions struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills kafka.topic.partitions metric with initial data.
func (m *metricKafkaTopicPartitions) init() {
	m.data.SetName("kafka.topic.partitions")
	m.data.SetDescription("Number of partitions in topic.")
	m.data.SetUnit("{partitions}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricKafkaTopicPartitions) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, topicAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntVal(val)
	dp.Attributes().PutString("topic", topicAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricKafkaTopicPartitions) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricKafkaTopicPartitions) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricKafkaTopicPartitions(settings MetricSettings) metricKafkaTopicPartitions {
	m := metricKafkaTopicPartitions{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

// MetricsBuilder provides an interface for scrapers to report metrics while taking care of all the transformations
// required to produce metric representation defined in metadata and user settings.
type MetricsBuilder struct {
	startTime                              pcommon.Timestamp   // start time that will be applied to all recorded data points.
	metricsCapacity                        int                 // maximum observed number of metrics per resource.
	resourceCapacity                       int                 // maximum observed number of resource attributes.
	metricsBuffer                          pmetric.Metrics     // accumulates metrics data before emitting.
	buildInfo                              component.BuildInfo // contains version information
	metricKafkaBrokers                     metricKafkaBrokers
	metricKafkaBrokersConsumerFetchRateAvg metricKafkaBrokersConsumerFetchRateAvg
	metricKafkaBrokersIncomingByteRateAvg  metricKafkaBrokersIncomingByteRateAvg
	metricKafkaBrokersOutgoingByteRateAvg  metricKafkaBrokersOutgoingByteRateAvg
	metricKafkaBrokersRequestLatencyAvg    metricKafkaBrokersRequestLatencyAvg
	metricKafkaBrokersRequestRateAvg       metricKafkaBrokersRequestRateAvg
	metricKafkaBrokersRequestSizeAvg       metricKafkaBrokersRequestSizeAvg
	metricKafkaBrokersRequestsInFlight     metricKafkaBrokersRequestsInFlight
	metricKafkaBrokersResponseRateAvg      metricKafkaBrokersResponseRateAvg
	metricKafkaBrokersResponseSizeAvg      metricKafkaBrokersResponseSizeAvg
	metricKafkaConsumerGroupLag            metricKafkaConsumerGroupLag
	metricKafkaConsumerGroupLagSum         metricKafkaConsumerGroupLagSum
	metricKafkaConsumerGroupMembers        metricKafkaConsumerGroupMembers
	metricKafkaConsumerGroupOffset         metricKafkaConsumerGroupOffset
	metricKafkaConsumerGroupOffsetSum      metricKafkaConsumerGroupOffsetSum
	metricKafkaPartitionCurrentOffset      metricKafkaPartitionCurrentOffset
	metricKafkaPartitionOldestOffset       metricKafkaPartitionOldestOffset
	metricKafkaPartitionReplicas           metricKafkaPartitionReplicas
	metricKafkaPartitionReplicasInSync     metricKafkaPartitionReplicasInSync
	metricKafkaTopicPartitions             metricKafkaTopicPartitions
}

// metricBuilderOption applies changes to default metrics builder.
type metricBuilderOption func(*MetricsBuilder)

// WithStartTime sets startTime on the metrics builder.
func WithStartTime(startTime pcommon.Timestamp) metricBuilderOption {
	return func(mb *MetricsBuilder) {
		mb.startTime = startTime
	}
}

func NewMetricsBuilder(settings MetricsSettings, buildInfo component.BuildInfo, options ...metricBuilderOption) *MetricsBuilder {
	mb := &MetricsBuilder{
		startTime:                              pcommon.NewTimestampFromTime(time.Now()),
		metricsBuffer:                          pmetric.NewMetrics(),
		buildInfo:                              buildInfo,
		metricKafkaBrokers:                     newMetricKafkaBrokers(settings.KafkaBrokers),
		metricKafkaBrokersConsumerFetchRateAvg: newMetricKafkaBrokersConsumerFetchRateAvg(settings.KafkaBrokersConsumerFetchRateAvg),
		metricKafkaBrokersIncomingByteRateAvg:  newMetricKafkaBrokersIncomingByteRateAvg(settings.KafkaBrokersIncomingByteRateAvg),
		metricKafkaBrokersOutgoingByteRateAvg:  newMetricKafkaBrokersOutgoingByteRateAvg(settings.KafkaBrokersOutgoingByteRateAvg),
		metricKafkaBrokersRequestLatencyAvg:    newMetricKafkaBrokersRequestLatencyAvg(settings.KafkaBrokersRequestLatencyAvg),
		metricKafkaBrokersRequestRateAvg:       newMetricKafkaBrokersRequestRateAvg(settings.KafkaBrokersRequestRateAvg),
		metricKafkaBrokersRequestSizeAvg:       newMetricKafkaBrokersRequestSizeAvg(settings.KafkaBrokersRequestSizeAvg),
		metricKafkaBrokersRequestsInFlight:     newMetricKafkaBrokersRequestsInFlight(settings.KafkaBrokersRequestsInFlight),
		metricKafkaBrokersResponseRateAvg:      newMetricKafkaBrokersResponseRateAvg(settings.KafkaBrokersResponseRateAvg),
		metricKafkaBrokersResponseSizeAvg:      newMetricKafkaBrokersResponseSizeAvg(settings.KafkaBrokersResponseSizeAvg),
		metricKafkaConsumerGroupLag:            newMetricKafkaConsumerGroupLag(settings.KafkaConsumerGroupLag),
		metricKafkaConsumerGroupLagSum:         newMetricKafkaConsumerGroupLagSum(settings.KafkaConsumerGroupLagSum),
		metricKafkaConsumerGroupMembers:        newMetricKafkaConsumerGroupMembers(settings.KafkaConsumerGroupMembers),
		metricKafkaConsumerGroupOffset:         newMetricKafkaConsumerGroupOffset(settings.KafkaConsumerGroupOffset),
		metricKafkaConsumerGroupOffsetSum:      newMetricKafkaConsumerGroupOffsetSum(settings.KafkaConsumerGroupOffsetSum),
		metricKafkaPartitionCurrentOffset:      newMetricKafkaPartitionCurrentOffset(settings.KafkaPartitionCurrentOffset),
		metricKafkaPartitionOldestOffset:       newMetricKafkaPartitionOldestOffset(settings.KafkaPartitionOldestOffset),
		metricKafkaPartitionReplicas:           newMetricKafkaPartitionReplicas(settings.KafkaPartitionReplicas),
		metricKafkaPartitionReplicasInSync:     newMetricKafkaPartitionReplicasInSync(settings.KafkaPartitionReplicasInSync),
		metricKafkaTopicPartitions:             newMetricKafkaTopicPartitions(settings.KafkaTopicPartitions),
	}
	for _, op := range options {
		op(mb)
	}
	return mb
}

// updateCapacity updates max length of metrics and resource attributes that will be used for the slice capacity.
func (mb *MetricsBuilder) updateCapacity(rm pmetric.ResourceMetrics) {
	if mb.metricsCapacity < rm.ScopeMetrics().At(0).Metrics().Len() {
		mb.metricsCapacity = rm.ScopeMetrics().At(0).Metrics().Len()
	}
	if mb.resourceCapacity < rm.Resource().Attributes().Len() {
		mb.resourceCapacity = rm.Resource().Attributes().Len()
	}
}

// ResourceMetricsOption applies changes to provided resource metrics.
type ResourceMetricsOption func(pmetric.ResourceMetrics)

// WithStartTimeOverride overrides start time for all the resource metrics data points.
// This option should be only used if different start time has to be set on metrics coming from different resources.
func WithStartTimeOverride(start pcommon.Timestamp) ResourceMetricsOption {
	return func(rm pmetric.ResourceMetrics) {
		var dps pmetric.NumberDataPointSlice
		metrics := rm.ScopeMetrics().At(0).Metrics()
		for i := 0; i < metrics.Len(); i++ {
			switch metrics.At(i).DataType() {
			case pmetric.MetricDataTypeGauge:
				dps = metrics.At(i).Gauge().DataPoints()
			case pmetric.MetricDataTypeSum:
				dps = metrics.At(i).Sum().DataPoints()
			}
			for j := 0; j < dps.Len(); j++ {
				dps.At(j).SetStartTimestamp(start)
			}
		}
	}
}

// EmitForResource saves all the generated metrics under a new resource and updates the internal state to be ready for
// recording another set of data points as part of another resource. This function can be helpful when one scraper
// needs to emit metrics from several resources. Otherwise calling this function is not required,
// just `Emit` function can be called instead.
// Resource attributes should be provided as ResourceMetricsOption arguments.
func (mb *MetricsBuilder) EmitForResource(rmo ...ResourceMetricsOption) {
	rm := pmetric.NewResourceMetrics()
	rm.Resource().Attributes().EnsureCapacity(mb.resourceCapacity)
	ils := rm.ScopeMetrics().AppendEmpty()
	ils.Scope().SetName("otelcol/kafkametricsreceiver")
	ils.Scope().SetVersion(mb.buildInfo.Version)
	ils.Metrics().EnsureCapacity(mb.metricsCapacity)
	mb.metricKafkaBrokers.emit(ils.Metrics())
	mb.metricKafkaBrokersConsumerFetchRateAvg.emit(ils.Metrics())
	mb.metricKafkaBrokersIncomingByteRateAvg.emit(ils.Metrics())
	mb.metricKafkaBrokersOutgoingByteRateAvg.emit(ils.Metrics())
	mb.metricKafkaBrokersRequestLatencyAvg.emit(ils.Metrics())
	mb.metricKafkaBrokersRequestRateAvg.emit(ils.Metrics())
	mb.metricKafkaBrokersRequestSizeAvg.emit(ils.Metrics())
	mb.metricKafkaBrokersRequestsInFlight.emit(ils.Metrics())
	mb.metricKafkaBrokersResponseRateAvg.emit(ils.Metrics())
	mb.metricKafkaBrokersResponseSizeAvg.emit(ils.Metrics())
	mb.metricKafkaConsumerGroupLag.emit(ils.Metrics())
	mb.metricKafkaConsumerGroupLagSum.emit(ils.Metrics())
	mb.metricKafkaConsumerGroupMembers.emit(ils.Metrics())
	mb.metricKafkaConsumerGroupOffset.emit(ils.Metrics())
	mb.metricKafkaConsumerGroupOffsetSum.emit(ils.Metrics())
	mb.metricKafkaPartitionCurrentOffset.emit(ils.Metrics())
	mb.metricKafkaPartitionOldestOffset.emit(ils.Metrics())
	mb.metricKafkaPartitionReplicas.emit(ils.Metrics())
	mb.metricKafkaPartitionReplicasInSync.emit(ils.Metrics())
	mb.metricKafkaTopicPartitions.emit(ils.Metrics())
	for _, op := range rmo {
		op(rm)
	}
	if ils.Metrics().Len() > 0 {
		mb.updateCapacity(rm)
		rm.MoveTo(mb.metricsBuffer.ResourceMetrics().AppendEmpty())
	}
}

// Emit returns all the metrics accumulated by the metrics builder and updates the internal state to be ready for
// recording another set of metrics. This function will be responsible for applying all the transformations required to
// produce metric representation defined in metadata and user settings, e.g. delta or cumulative.
func (mb *MetricsBuilder) Emit(rmo ...ResourceMetricsOption) pmetric.Metrics {
	mb.EmitForResource(rmo...)
	metrics := pmetric.NewMetrics()
	mb.metricsBuffer.MoveTo(metrics)
	return metrics
}

// RecordKafkaBrokersDataPoint adds a data point to kafka.brokers metric.
func (mb *MetricsBuilder) RecordKafkaBrokersDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricKafkaBrokers.recordDataPoint(mb.startTime, ts, val)
}

// RecordKafkaBrokersConsumerFetchRateAvgDataPoint adds a data point to kafka.brokers.consumer_fetch_rate_avg metric.
func (mb *MetricsBuilder) RecordKafkaBrokersConsumerFetchRateAvgDataPoint(ts pcommon.Timestamp, val float64, brokerAttributeValue string) {
	mb.metricKafkaBrokersConsumerFetchRateAvg.recordDataPoint(mb.startTime, ts, val, brokerAttributeValue)
}

// RecordKafkaBrokersIncomingByteRateAvgDataPoint adds a data point to kafka.brokers.incoming_byte_rate_avg metric.
func (mb *MetricsBuilder) RecordKafkaBrokersIncomingByteRateAvgDataPoint(ts pcommon.Timestamp, val float64, brokerAttributeValue string) {
	mb.metricKafkaBrokersIncomingByteRateAvg.recordDataPoint(mb.startTime, ts, val, brokerAttributeValue)
}

// RecordKafkaBrokersOutgoingByteRateAvgDataPoint adds a data point to kafka.brokers.outgoing_byte_rate_avg metric.
func (mb *MetricsBuilder) RecordKafkaBrokersOutgoingByteRateAvgDataPoint(ts pcommon.Timestamp, val float64, brokerAttributeValue string) {
	mb.metricKafkaBrokersOutgoingByteRateAvg.recordDataPoint(mb.startTime, ts, val, brokerAttributeValue)
}

// RecordKafkaBrokersRequestLatencyAvgDataPoint adds a data point to kafka.brokers.request_latency_avg metric.
func (mb *MetricsBuilder) RecordKafkaBrokersRequestLatencyAvgDataPoint(ts pcommon.Timestamp, val float64, brokerAttributeValue string) {
	mb.metricKafkaBrokersRequestLatencyAvg.recordDataPoint(mb.startTime, ts, val, brokerAttributeValue)
}

// RecordKafkaBrokersRequestRateAvgDataPoint adds a data point to kafka.brokers.request_rate_avg metric.
func (mb *MetricsBuilder) RecordKafkaBrokersRequestRateAvgDataPoint(ts pcommon.Timestamp, val float64, brokerAttributeValue string) {
	mb.metricKafkaBrokersRequestRateAvg.recordDataPoint(mb.startTime, ts, val, brokerAttributeValue)
}

// RecordKafkaBrokersRequestSizeAvgDataPoint adds a data point to kafka.brokers.request_size_avg metric.
func (mb *MetricsBuilder) RecordKafkaBrokersRequestSizeAvgDataPoint(ts pcommon.Timestamp, val float64, brokerAttributeValue string) {
	mb.metricKafkaBrokersRequestSizeAvg.recordDataPoint(mb.startTime, ts, val, brokerAttributeValue)
}

// RecordKafkaBrokersRequestsInFlightDataPoint adds a data point to kafka.brokers.requests_in_flight metric.
func (mb *MetricsBuilder) RecordKafkaBrokersRequestsInFlightDataPoint(ts pcommon.Timestamp, val int64, brokerAttributeValue string) {
	mb.metricKafkaBrokersRequestsInFlight.recordDataPoint(mb.startTime, ts, val, brokerAttributeValue)
}

// RecordKafkaBrokersResponseRateAvgDataPoint adds a data point to kafka.brokers.response_rate_avg metric.
func (mb *MetricsBuilder) RecordKafkaBrokersResponseRateAvgDataPoint(ts pcommon.Timestamp, val float64, brokerAttributeValue string) {
	mb.metricKafkaBrokersResponseRateAvg.recordDataPoint(mb.startTime, ts, val, brokerAttributeValue)
}

// RecordKafkaBrokersResponseSizeAvgDataPoint adds a data point to kafka.brokers.response_size_avg metric.
func (mb *MetricsBuilder) RecordKafkaBrokersResponseSizeAvgDataPoint(ts pcommon.Timestamp, val float64, brokerAttributeValue string) {
	mb.metricKafkaBrokersResponseSizeAvg.recordDataPoint(mb.startTime, ts, val, brokerAttributeValue)
}

// RecordKafkaConsumerGroupLagDataPoint adds a data point to kafka.consumer_group.lag metric.
func (mb *MetricsBuilder) RecordKafkaConsumerGroupLagDataPoint(ts pcommon.Timestamp, val int64, groupAttributeValue string, topicAttributeValue string, partitionAttributeValue int64) {
	mb.metricKafkaConsumerGroupLag.recordDataPoint(mb.startTime, ts, val, groupAttributeValue, topicAttributeValue, partitionAttributeValue)
}

// RecordKafkaConsumerGroupLagSumDataPoint adds a data point to kafka.consumer_group.lag_sum metric.
func (mb *MetricsBuilder) RecordKafkaConsumerGroupLagSumDataPoint(ts pcommon.Timestamp, val int64, groupAttributeValue string, topicAttributeValue string) {
	mb.metricKafkaConsumerGroupLagSum.recordDataPoint(mb.startTime, ts, val, groupAttributeValue, topicAttributeValue)
}

// RecordKafkaConsumerGroupMembersDataPoint adds a data point to kafka.consumer_group.members metric.
func (mb *MetricsBuilder) RecordKafkaConsumerGroupMembersDataPoint(ts pcommon.Timestamp, val int64, groupAttributeValue string) {
	mb.metricKafkaConsumerGroupMembers.recordDataPoint(mb.startTime, ts, val, groupAttributeValue)
}

// RecordKafkaConsumerGroupOffsetDataPoint adds a data point to kafka.consumer_group.offset metric.
func (mb *MetricsBuilder) RecordKafkaConsumerGroupOffsetDataPoint(ts pcommon.Timestamp, val int64, groupAttributeValue string, topicAttributeValue string, partitionAttributeValue int64) {
	mb.metricKafkaConsumerGroupOffset.recordDataPoint(mb.startTime, ts, val, groupAttributeValue, topicAttributeValue, partitionAttributeValue)
}

// RecordKafkaConsumerGroupOffsetSumDataPoint adds a data point to kafka.consumer_group.offset_sum metric.
func (mb *MetricsBuilder) RecordKafkaConsumerGroupOffsetSumDataPoint(ts pcommon.Timestamp, val int64, groupAttributeValue string, topicAttributeValue string) {
	mb.metricKafkaConsumerGroupOffsetSum.recordDataPoint(mb.startTime, ts, val, groupAttributeValue, topicAttributeValue)
}

// RecordKafkaPartitionCurrentOffsetDataPoint adds a data point to kafka.partition.current_offset metric.
func (mb *MetricsBuilder) RecordKafkaPartitionCurrentOffsetDataPoint(ts pcommon.Timestamp, val int64, topicAttributeValue string, partitionAttributeValue int64) {
	mb.metricKafkaPartitionCurrentOffset.recordDataPoint(mb.startTime, ts, val, topicAttributeValue, partitionAttributeValue)
}

// RecordKafkaPartitionOldestOffsetDataPoint adds a data point to kafka.partition.oldest_offset metric.
func (mb *MetricsBuilder) RecordKafkaPartitionOldestOffsetDataPoint(ts pcommon.Timestamp, val int64, topicAttributeValue string, partitionAttributeValue int64) {
	mb.metricKafkaPartitionOldestOffset.recordDataPoint(mb.startTime, ts, val, topicAttributeValue, partitionAttributeValue)
}

// RecordKafkaPartitionReplicasDataPoint adds a data point to kafka.partition.replicas metric.
func (mb *MetricsBuilder) RecordKafkaPartitionReplicasDataPoint(ts pcommon.Timestamp, val int64, topicAttributeValue string, partitionAttributeValue int64) {
	mb.metricKafkaPartitionReplicas.recordDataPoint(mb.startTime, ts, val, topicAttributeValue, partitionAttributeValue)
}

// RecordKafkaPartitionReplicasInSyncDataPoint adds a data point to kafka.partition.replicas_in_sync metric.
func (mb *MetricsBuilder) RecordKafkaPartitionReplicasInSyncDataPoint(ts pcommon.Timestamp, val int64, topicAttributeValue string, partitionAttributeValue int64) {
	mb.metricKafkaPartitionReplicasInSync.recordDataPoint(mb.startTime, ts, val, topicAttributeValue, partitionAttributeValue)
}

// RecordKafkaTopicPartitionsDataPoint adds a data point to kafka.topic.partitions metric.
func (mb *MetricsBuilder) RecordKafkaTopicPartitionsDataPoint(ts pcommon.Timestamp, val int64, topicAttributeValue string) {
	mb.metricKafkaTopicPartitions.recordDataPoint(mb.startTime, ts, val, topicAttributeValue)
}

// Reset resets metrics builder to its initial state. It should be used when external metrics source is restarted,
// and metrics builder should update its startTime and reset it's internal state accordingly.
func (mb *MetricsBuilder) Reset(options ...metricBuilderOption) {
	mb.startTime = pcommon.NewTimestampFromTime(time.Now())
	for _, op := range options {
		op(mb)
	}
}
